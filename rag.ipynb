{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A pipeline with Retrieval Augmented Generation (RAG) \n",
    "\n",
    "### Bence Csabai - KPMG Machine Learning Engineer interview\n",
    "\n",
    "The following notebook serves as a demo for a small project for the KPMG Machine Learning position take home assignment. The task was to create a chatbot that answers mock \"business inquiries\", with the following instructions:\n",
    "- The solution should be based on a RAG (Retreival Augmented Generation model) which consists of\n",
    "    - a Retriever layer\n",
    "    - an LLM Layer\n",
    "- The suggested [dataset](https://huggingface.co/datasets/wikipedia/viewer/20220301.en) to use for the Retriever is the English wikipedia dataset from HuggingFace. Instead I used its smaller brother, the simplified Wikipedia [dataset](https://huggingface.co/datasets/wikipedia/viewer/20220301.simple)\n",
    "- \"Business inquiries\" for this example should be considered anything a person might ask from e.g. the Wikipedia dataset\n",
    "\n",
    "### RAG\n",
    "\n",
    "LLMs have proven to be powerful and versatile tools for a broad range of applications. They are however (as their name suggests) very large, therefore training them for a new specific task from scratch requires tremendous amount of computational power, meaning a lot of time and resources would have to be invested. One of the ways to overcome this issue is to use a pretrained model, and tailor it to our needs. A method for this is using Retrieval Augmented Generation, or RAG models. RAGs consist of two main blocks: the retriever and the LLM block.\n",
    "\n",
    "![RAG Structure](F:/Users/bence/Documents/_PROJECTS/KPMG_LLM/large-language-models/rag/rag_s.png)\n",
    "\n",
    "**Retriever block**\n",
    "\n",
    "The aim of the retriever block is to create a richer prompt for the LLM block by adding context to the original query. It achieves this by having its own knowledge database. In this case this is the simplified wikipedia database from HuggingFace. It then converts this text based database into vector representations with an embedding network (in this case a pretrained transformer model, which transforms the text into a 384 dimensional vector space). The original prompt/query is also passed through the same embedding network, ensuring that the knowledge base and the query will be in the same representation space as the data. \n",
    "\n",
    "Afterwards, the closest *k* vectors to the query vector are found and selected as context. This is done using the Facebook AI Similarity Search (FAISS) which is a library that is optimized to find nearest neighbors fast. The corresponding texts from the document dataset are then added to a template prompt (e.g. \"Consider the following context and then answer the question: *context*, *question*\")\n",
    "\n",
    "**LLM block**\n",
    "\n",
    "The LLM block, as its name suggests it contains a pretrained LLM, which answers the enriched prompt. In this example, I used a pretrained Llama 2 model (the open source LLM model developed by Meta). The LLM is expected to produce a better quality result then the base LLM. This is tested at the end of the notebook\n",
    "\n",
    "### How to use? \n",
    "\n",
    "- Enter a query in the appropriate cell (QUERY variable)\n",
    "- Run all the code\n",
    "- **NOTES**: the RAG model can take a few minutes to produce a result, as it gets a long prompt and the model runs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "from encoder.encoder import Encoder\n",
    "from generator.generator import Generator\n",
    "from retriever.vector_db import VectorDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell contains the template for the enhanced prompt: {context} gets replaced with the chosen data from the Wikipedia dataset, while {question} gets replaced with the original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Anaconda3\\envs\\LangChain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "F:\\ProgramData\\Anaconda3\\envs\\LangChain\\lib\\site-packages\\datasets\\load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load wikipedia dataset\n",
    "loader = HuggingFaceDatasetLoader(\"wikipedia\", name=\"20220301.simple\")\n",
    "docs = loader.load()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "#set device to cuda (just in case)\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# initiate our classes for the Encoder, Retriever and Generator\n",
    "encoder = Encoder()\n",
    "faiss_db = VectorDatabase()\n",
    "generator = Generator(TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"When is April Fools day?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY_USER = input('What would you like to know?\\n')\n",
    "#QUERY = QUERY_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find \n",
    "Convert docs and query to vector space and find the closest neighbors of the query using the FAISS library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create passages of same length from documents\n",
    "# This should be able to split documents, but I do not think I got it to work correctly, \n",
    "# therefore the whole articles get represented in the vector space\n",
    "\n",
    "passages = faiss_db.create_passages(docs)\n",
    "faiss_db.store_passages_db(passages, encoder.encoder)\n",
    "\n",
    "\n",
    "# retrieve the k most similar documents to our query\n",
    "\n",
    "context = faiss_db.retrieve_most_similar_document(QUERY, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers - comparison\n",
    "\n",
    "Get results from both the RAG model, and the base LLaMa model and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April Fools day, also known as All Fools Day or Poisson dâ€™Avril in French, occurs every year on the first of April. It is a lighthearted day when people play practical jokes and hoaxes on each other, often making fun of someone else's gullibility or naivety. The origins of this holiday are uncertain, but one theory suggests that it stems from the custom in France during the 18th century to celebrate the New Year with practical jokes and pranks on April 1st. Another theory is that it comes from the adoption of Christianity, where the first day of April was considered a sacred day and thus unlucky for starting new projects or initiatives. Regardless of its origins, many people around the world celebrate April Fools Day by playing pranks on each other, such as sending fake news stories to their friends and family or creating elaborate hoaxes that they hope will be revealed as such at the end of the day. Some people even plan elaborate prank events like an April Fools' Day festival or parade with floats and costumes designed specifically for this occasion. Overall, April Fools Day is a fun way to lighten up the mood and enjoy some harmless joking around with friends and family members!\n"
     ]
    }
   ],
   "source": [
    "# RAG response\n",
    "print(generator.get_answer(context[:4096], QUERY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) April Fools Day is a holiday in many countries, including the United States and Canada, where people play practical jokes on one another and perform pranks as a way of celebrating the beginning of spring.\n",
      "2) It takes place annually on April 1st.\n"
     ]
    }
   ],
   "source": [
    "# Base LLM model\n",
    "print(generator.get_answer('', QUERY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future possibilities\n",
    "\n",
    "While the model produces an acceptable rewards and we can observe instances where the RAG model yields better answers than the base LLM, there are still a number of changes that could be made to further improve the quality. Limitations for this solution included lack of computing power and disk space (only the first 100 instances of the simplified dataset are used), and limited time for tweaking and testing. The following is a non exhaustive list of some of the ideas, that could be done, given more time and resources:\n",
    "\n",
    "- **Model experimenting**\n",
    "    - Try different versions of different open source LLM models (e.g. different Llama versions, Falcon, Yi, etc.) and compare speed and quality\n",
    "    - Try different models for encoding\n",
    "    - Try different ranking models, not just necessarily nearest neighbors (could be smaller LLM based?)\n",
    "- **Hyperparameter tuning**\n",
    "    - Change different non-learnable parameters, such as\n",
    "        - The length of the passages - Doc encoded by few sentences instead of article? This would probably increase the time taken to create the representation but would probably speed the answer generation therefore increasing user experience. Does this affect quality?\n",
    "        -  How does number of nearest neighbors affect speed and quality?\n",
    "- **Scale and Speed**\n",
    "    -  Find ways to speed up the answer generation\n",
    "        - Provide shorter contexts for shorter overall prompts?\n",
    "        - Create multiple smaller retrievers with different context groups?\n",
    "        - Other  \n",
    "    -  Try the model with more powerful hardware -> able to load more data, generate response quicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "Most of the project was based on the following github repo:\n",
    "https://github.com/zaai-ai/large-language-models/blob/main/rag/\n",
    "\n",
    "The base skeleton and structure of the model remains. I spent most of my time diving deeper and understanding the model's theory rather than optimizing the code. However there were many changes needed to be made to configure the solution for the task at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
