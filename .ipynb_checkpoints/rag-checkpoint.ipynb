{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A pipeline with Retrieval Augmented Generation (RAG) \n",
    "\n",
    "The following notebook serves as a demo for a small project for the KPMG Machine Learning position take home assignment. The task was to create a chatbot that answers mock \"business inquiries\", with the following instructions:\n",
    "- The solution should be based on a RAG (Retreival Augmented Generation model) which consists of\n",
    "    - a Retriever layer\n",
    "    - an LLM Layer\n",
    "- The suggested [dataset](https://huggingface.co/datasets/wikipedia/viewer/20220301.en) to use for the Retriever is the English wikipedia dataset from HuggingFace. Instead I used its smaller brother, the simplified Wikipedia [dataset](https://huggingface.co/datasets/wikipedia/viewer/20220301.simple)\n",
    "- \"Business inquiries\" for this example should be considered anything a person might ask from e.g. the Wikipedia dataset\n",
    "\n",
    "### RAG\n",
    "\n",
    "LLMs have proven to be powerful and versatile tools for a broad range of applications. They are however (as their name suggests) very large, therefore training them for a new specific task from scratch requires tremendous amount of computational power, meaning a lot of time and resources would have to be invested. One of the ways to overcome this issue is to use a pretrained model, and tailor it to our needs. A method for this is using Retrieval Augmented Generation, or RAG models. RAGs consist of two main blocks: the retriever and the LLM block.\n",
    "\n",
    "![RAG Structure](F:/Users/bence/Documents/_PROJECTS/KPMG_LLM/large-language-models/rag/rag_s.png)\n",
    "\n",
    "**Retriever block**\n",
    "\n",
    "The aim of the retriever block is to create a richer prompt for the LLM block by adding context to the original query. It achieves this by having its own knowledge database. In this case this is the simplified wikipedia database from HuggingFace. It then converts this text based database into vector representations with an embedding network (in this case a pretrained transformer model, which transforms the text into a 384 dimensional vector space). The original prompt/query is also passed through the same embedding network, ensuring that the knowledge base and the query will be in the same representation space as the data. \n",
    "\n",
    "Afterwards, the closest *k* vectors to the query vector are found and selected as context. The corresponding texts from the document dataset are then added to a template prompt (e.g. \"Consider the following context and then answer the question: *context*, *question*\")\n",
    "\n",
    "**LLM block**\n",
    "\n",
    "The LLM block, as its name suggests it contains a pretrained LLM, which answers the enriched prompt. In this example, I used a pretrained Llama 2 model (the open source LLM model developed by Meta). The LLM is expected to produce a better quality result then the base LLM. This is tested at the end of the notebook\n",
    "\n",
    "### How to use? \n",
    "\n",
    "- Enter a query in the appropriate cell (QUERY variable)\n",
    "- Run all the code\n",
    "- **NOTES**: the RAG model can take a few minutes to produce a result, as it gets a long prompt and the model runs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "from encoder.encoder import Encoder\n",
    "from generator.generator import Generator\n",
    "from retriever.vector_db import VectorDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following cell contains the template for the enhanced prompt: {context} gets replaced with the chosen data from the Wikipedia dataset, while {question} gets replaced with the original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Anaconda3\\envs\\LangChain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "F:\\ProgramData\\Anaconda3\\envs\\LangChain\\lib\\site-packages\\datasets\\load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load wikipedia dataset\n",
    "loader = HuggingFaceDatasetLoader(\"wikipedia\", name=\"20220301.simple\")\n",
    "docs = loader.load()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set device to cuda (just in case)\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate our classes for the Encoder, Retriever and Generator\n",
    "encoder = Encoder()\n",
    "faiss_db = VectorDatabase()\n",
    "generator = Generator(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create passages of same length from documents\n",
    "# This should be able to split documents, but I do not think I got it to work correctly, \n",
    "# therefore the whole articles get represented in the vector space\n",
    "passages = faiss_db.create_passages(docs)\n",
    "faiss_db.store_passages_db(passages, encoder.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"When is April Fools day?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY_USER = input('What would you like to know?\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find \n",
    "Convert query to vector space and find the closest docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve the k most similar documents to our query\n",
    "context = faiss_db.retrieve_most_similar_document(QUERY, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers - comparison\n",
    "\n",
    "Get results from both the RAG model, and the base LLaMa model and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG response\n",
    "print(generator.get_answer(context[:4096], QUERY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base LLM model\n",
    "print(generator.get_answer('', QUERY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
